# import logging
# import os

# import hydra
# from omegaconf import DictConfig
# from models import MODELS
# from data_loader import get_dataset
# from factory.trainer import Trainer
# from factory.evaluator import Evaluator
# from factory.profit_calculator import ProfitCalculator
# import pandas as pd

# from sklearn.model_selection import TimeSeriesSplit
# from path_definition import HYDRA_PATH

# from utils.reporter import Reporter
# from data_loader.creator import create_dataset, preprocess



# import yaml
# from datetime import datetime, timedelta

# import requests
# import csv
# import os
# import pytz
# from datetime import datetime, timedelta
# logger = logging.getLogger(__name__)
# import pandas as pd

# # Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙØ§ØµÙŠÙ„ API
# url = "https://api.binance.com/api/v3/klines"
# symbols = ['ETHBTC', 'LTCBTC', 'BNBBTC', 'NEOBTC', 'QTUMETH', 'EOSETH', 'SNTETH', 'BNTETH', 'BCCBTC', 'GASBTC', 'BNBETH', 'BTCUSDT']
# symbols = ['ETHBTC','BTCUSDT']

# # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª


# # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø­Ø§Ù„ÙŠ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
# current_date = datetime.now()

# data = {
#     "window_size": 5,
#     "train_start_date": "2022-01-01 13:30:00",  # ØªØ§Ø±ÙŠØ® Ø«Ø§Ø¨Øª
#     "train_end_date": (current_date - timedelta(days=5)).strftime("%Y-%m-%d 09:30:00"),
#     "valid_start_date": (current_date - timedelta(days=5)).strftime("%Y-%m-%d 10:30:00"),
#     "valid_end_date": (current_date + timedelta(days=2)).strftime("%Y-%m-%d 10:30:00"),
#     "features": "Date, open, High, Low, close, volume",
#     "indicators_names": "rsi macd"
# }

# # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„Ù YAML Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ
# with open("configs/hydra/dataset_loader/common.yaml", "w") as file:
#     yaml.dump(data, file, default_flow_style=False, allow_unicode=True, sort_keys=False)



# def check_and_delete_file(filename):
#     try:
#         with open(filename, 'r') as file:
#             last_line = file.readlines()[-1]  # Read the last line
#             last_date = datetime.strptime(last_line.split(',')[0], '%Y-%m-%d %H:%M:%S%z')
#             # Check if the data covers up to yesterday
#             if last_date.date() < (datetime.now(pytz.utc).date() - timedelta(days=1)):
#                 os.remove(filename)  # Delete file if data is outdated
#                 print(f"File {filename} deleted because it doesn't cover up to yesterday.")
#     except Exception as e:
#         print(f"An error occurred while checking file {filename}:Â {e}")

# def add_future_dates(filename, symbol):
#     today = datetime.now(pytz.utc).replace(hour=0, minute=0, second=0, microsecond=0)
#     future_dates = [today + timedelta(days=i) for i in range(3)]  # today and the next two days

#     with open(filename, 'a', newline='') as file:
#         writer = csv.writer(file)
#         for future_date in future_dates:
#             formatted_date = future_date.strftime('%Y-%m-%d 00:00:00+00:00')
#             writer.writerow([formatted_date, symbol,'1','1','1','1','1'])


# data_folder = 'C:/Users/moham/Downloads/crypto/crypto/data'

# if not os.path.exists(data_folder):
#     os.makedirs(data_folder)

# data_filename = os.path.join(data_folder,'data1.csv')

# def fetch_and_save_data(symbol, start_date, end_date):
#     url = "https://api.binance.com/api/v3/klines"  # Add API URL here
#     params = {
#         'symbol': symbol,
#         'interval': '1d',
#         'startTime': int(start_date.timestamp() * 1000),
#         'endTime': int(end_date.timestamp() * 1000)
#     }
    
#     response = requests.get(url, params=params)
#     data = response.json()

#     # Check if data is available
#     if not data:
#         print(f"No data available for {symbol} from {start_date.date()}")
#         return False  # No data for this symbol

#     # Write data to the file
#     with open(data_filename, 'a', newline='') as file:
#         writer = csv.writer(file)
#         for entry in data:
#             timestamp = datetime.fromtimestamp(entry[0] / 1000, tz=pytz.utc).strftime('%Y-%m-%d 00:00:00+00:00')
#             writer.writerow([
#                 timestamp, symbol, entry[1], entry[2], entry[3], entry[4], entry[5]
#             ])
    
#     return True  # Data fetchedÂ successfully

# @hydra.main(config_path=HYDRA_PATH, config_name="train")
# def train(cfg: DictConfig):
#     start_date = datetime(2022, 1, 1, tzinfo=pytz.utc)
#     end_date = datetime.now(pytz.utc) - timedelta(days=1)
#     period = timedelta(days=90)

#     title=''
  
#     for symbol in symbols:
#         title+=f'Ø±Ù…Ø² Ø§Ù„Ø¹Ù…Ù„Ø©: {symbol}ğŸª™\n'
        
#         # //
#         if os.path.exists(data_filename):
#             os.remove(data_filename)  # Delete the file if it exists

#         # Write headers only when creating the file
#         with open(data_filename, 'a', newline='') as file:
#             writer = csv.writer(file)
#             writer.writerow(['timestamp', 'symbol', 'open', 'high', 'low', 'close', 'volume'])

#         current_start = start_date
#         data_available = False

#         while current_start < end_date:
#             current_end = min(current_start + period, end_date)
#             print(f"Fetching data for {symbol} from {current_start.date()} to {current_end.date()}")
        
#         # Fetch data for each period
#             if fetch_and_save_data(symbol, current_start, current_end):
#                 data_available = True  # Data fetched successfully for at least one period

#             current_start = current_end + timedelta(days=1)
    
#         if not data_available:
#             print(f"Skipping {symbol} due to insufficient data.")
#             continue  # Skip symbol if no data is available

#         # Add three future dates at the end of the file with the symbol
#         add_future_dates(data_filename, symbol)

#         print("Data download complete for all symbols with data from the beginning of 2022.")
#         # //
#         if cfg.load_path is None and cfg.model is None:
#             msg = 'either specify a load_path or config a model.'
#             logger.error(msg)
#             raise Exception(msg)

#         elif cfg.load_path is not None:
#             dataset_ = pd.read_csv(cfg.load_path)
#             if 'Date' not in dataset_.keys():
#                 dataset_.rename(columns={'timestamp': 'Date'}, inplace=True)
#             if 'High' not in dataset_.keys():
#                 dataset_.rename(columns={'high': 'High'}, inplace=True)
#             if 'Low' not in dataset_.keys():
#                 dataset_.rename(columns={'low': 'Low'}, inplace=True)

#             dataset, profit_calculator = preprocess(dataset_, cfg, logger)

#         elif cfg.model is not None:
#             dataset, profit_calculator = get_dataset(cfg.dataset_loader.name, cfg.dataset_loader.train_start_date,
#                                 cfg.dataset_loader.valid_end_date, cfg)

#         cfg.save_dir = os.getcwd()
#         reporter = Reporter(cfg)
#         reporter.setup_saving_dirs(cfg.save_dir)
#         model = MODELS[cfg.model.type](cfg.model)

#         dataset_for_profit = dataset.copy()
#         dataset_for_profit.drop(['prediction'], axis=1, inplace=True)
#         dataset.drop(['predicted_high', 'predicted_low'], axis=1, inplace=True)
     
#         if cfg.validation_method == 'simple':
#             train_dataset = dataset[
#                 (dataset['Date'] > cfg.dataset_loader.train_start_date) & (
#                             dataset['Date'] < cfg.dataset_loader.train_end_date)]
#             valid_dataset = dataset[
#                 (dataset['Date'] > cfg.dataset_loader.valid_start_date) & (
#                             dataset['Date'] < cfg.dataset_loader.valid_end_date)]
#             Trainer(cfg, train_dataset, None, model).train()
#             mean_prediction = Evaluator(cfg, test_dataset=valid_dataset, model=model, reporter=reporter).evaluate()
#             print('..............................d')
#             print(mean_prediction)
#         elif cfg.validation_method == 'cross_validation':
#             n_split = 3
#             tscv = TimeSeriesSplit(n_splits=n_split)

#             for train_index, test_index in tscv.split(dataset):
#                 train_dataset, valid_dataset = dataset.iloc[train_index], dataset.iloc[test_index]
#                 Trainer(cfg, train_dataset, None, model).train()
#                 mean_prediction = Evaluator(cfg, test_dataset=valid_dataset, model=model, reporter=reporter).evaluate()

#             reporter.add_average()

#         x=ProfitCalculator(cfg, dataset_for_profit, profit_calculator, mean_prediction, reporter).profit_calculator()
        
#         predicted_high=x[0]['predicted_high']
#         predicted_low=x[0]['predicted_low']
#         predicted_mean=x[0]['predicted_mean']
#         title+=f'Ø§Ø¹Ù„Ù‰ Ø³Ø¹Ø± Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„ÙŠÙˆÙ…: {predicted_high}â¬†ï¸ğŸ”®\n'
#         title+=f'Ø§Ù‚Ù„ Ø³Ø¹Ø± Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„ÙŠÙˆÙ…: {predicted_low}â¬‡ï¸ğŸ”®\n'
#         title+=f'Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„ÙŠÙˆÙ…: {predicted_mean}ğŸ”®\n'
#         title+='----------------------------------------'
#         print(title)

#         reporter.print_pretty_metrics(logger)
#         reporter.save_metrics()
#     title+='Ù„Ø§ ØªØ¬Ø¹Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ø­ÙˆØ± ØªØ¯Ø§ÙˆÙ„Ùƒ. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±ØŒ ÙˆØ§Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ù„Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø±Ø§Øª Ù…Ø³ØªÙ†ÙŠØ±Ø©.\n'
#     print(title)


# if __name__ == '__main__':
#     train()
import logging
import os

import hydra
from omegaconf import DictConfig
from models import MODELS
from data_loader import get_dataset
from factory.trainer import Trainer
from factory.evaluator import Evaluator
from factory.profit_calculator import ProfitCalculator
import pandas as pd

from sklearn.model_selection import TimeSeriesSplit
from path_definition import HYDRA_PATH

from utils.reporter import Reporter
from data_loader.creator import create_dataset, preprocess



import yaml
from datetime import datetime, timedelta

import requests
import csv
import os
import pytz
from datetime import datetime, timedelta
logger = logging.getLogger(__name__)
import pandas as pd


# Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙØ§ØµÙŠÙ„ API
url = "https://api.binance.com/api/v3/klines"

symbols  = [
  "BTCUSDT", "ETHUSDT", "BNBUSDT", "XRPUSDT", "DOGEUSDT", "ADAUSDT", 
  "SOLUSDT", "DOTUSDT", "LTCUSDT", "LINKUSDT", "XLMUSDT", "BCHUSDT", 
  "TRXUSDT", "VETUSDT", "EOSUSDT", "XMRUSDT", "XTZUSDT", "THETAUSDT", 
  "ATOMUSDT", "ALGOUSDT", "FILUSDT", "ZECUSDT", "NEOUSDT",
  "DASHUSDT", "KSMUSDT", "XEMUSDT", "QTUMUSDT", "SNXUSDT", "MANAUSDT", 
  "AAVEUSDT", "MKRUSDT", "COMPUSDT", "CHZUSDT", "ENJUSDT", "SUSHIUSDT", 
  "YFIUSDT", "CRVUSDT", "ZRXUSDT", "FTMUSDT", "BNTUSDT", "RENUSDT", 
  "SRMUSDT", "BALUSDT", "BATUSDT", "CELOUSDT", "EGLDUSDT", "ONEUSDT", 
  "KAVAUSDT", "LUNAUSDT", "OCEANUSDT", "ICXUSDT", "RSRUSDT", "NEXOUSDT", 
  "POWRUSDT", "OGNUSDT", "SNTUSDT", "REEFUSDT", "ANKRUSDT", "NEARUSDT", 
  "PUNDIXUSDT", "KEEPUSDT", "QNTUSDT", "HBARUSDT", "AVAXUSDT", "ONTUSDT", 
  "HOTUSDT", "SCUSDT", "ANTUSDT", "EWTUSDT", "SXPUSDT", "LSKUSDT", 
  "OXTUSDT", "STORJUSDT", "USTUSDT", "RUNEUSDT", "AMPLUSDT", "CVCUSDT", 
  "FUNUSDT", "INJUSDT", "SKLUSDT", "CKBUSDT", "ARKUSDT", "FETUSDT", 
  "DGUSDT", "CELRUSDT", "AKROUSDT", "AKTUSDT", "AERGOUSDT", "BLZUSDT", 
  "CSPRUSDT", "CTKUSDT", "CTSIUSDT", "DUSKUSDT", "ALPHAUSDT", "AUDIOUSDT", 
  "BADGERUSDT", "BELUSDT", "BONDUSDT", "KP3RUSDT", "MATICUSDT", "IMXUSDT", 
  "SRMUSDT", "TVKUSDT", "DEGOUNDT", "PLUUSDT", "LTOUSDT", "TOMOUSDT", 
  "WANUSDT", "JSTUSDT", "LITUSDT", "ORNUSDT", "PHAUSDT", "RADUSDT", 
  "RGTUSDT", "RAMPUSDT", "REQUSDT", "RNDRUSDT", "RSVUSDT", "AGIXUSDT", 
  "SFIUSDT", "SANDUSDT", "SDTUSDT", "SLPUSDT", "STEEMUSDT", "TRBUSDT", 
  "TTUSDT", "TRYUSDT", "TRUUSDT", "TWTUSDT", "UOSUSDT", "UNFIUSDT", 
  "VELOUSDT", "VRAUSDT", "VGXUSDT", "WABIUSDT", "WOOUSDT", "WINGUSDT", 
  "WRXUSDT", "YGGUSDT", "YOYOWUSDT", "ZILUSDT", "MPHUSDT", "ALEPHUSDT", 
  "AMBUSDT", "FIDAUSDT", "CREAMUSDT", "DGBUSDT", "DENTUSDT", "DODOUSDT"
];




# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª


# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø­Ø§Ù„ÙŠ ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
current_date = datetime.now()

data = {
    "window_size": 5,
    "train_start_date": "2020-01-01 13:30:00",  # ØªØ§Ø±ÙŠØ® Ø«Ø§Ø¨Øª
    "train_end_date": (current_date - timedelta(days=5)).strftime("%Y-%m-%d 09:30:00"),
    "valid_start_date": (current_date - timedelta(days=5)).strftime("%Y-%m-%d 10:30:00"),
    "valid_end_date": (current_date + timedelta(days=2)).strftime("%Y-%m-%d 10:30:00"),
    "features": "Date, open, High, Low, close, volume",
    "indicators_names": "rsi macd"
}

# ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ù…Ù„Ù YAML Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªÙ†ØµÙŠØµ
with open("configs/hydra/dataset_loader/common.yaml", "w") as file:
    yaml.dump(data, file, default_flow_style=False, allow_unicode=True, sort_keys=False)



def check_and_delete_file(filename):
    try:
        # ÙØªØ­ Ø§Ù„Ù…Ù„Ù ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¢Ø®Ø± Ø³Ø·Ø±
        with open(filename, 'r') as file:
            lines = file.readlines()
            if not lines:
                print(f"No data in file {filename}.")
                os.remove(filename)
                return None, False

            last_line = lines[-1]  # Ù‚Ø±Ø§Ø¡Ø© Ø¢Ø®Ø± Ø³Ø·Ø± ÙÙŠ Ø§Ù„Ù…Ù„Ù
            last_date = datetime.strptime(last_line.split(',')[0], '%Y-%m-%d %H:%M:%S%z')
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§Ù…Ù„Ø© Ø­ØªÙ‰ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ù…Ø³
            if last_date.date() < (datetime.now(pytz.utc).date() - timedelta(days=1)):
                os.remove(filename)  # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¯ÙŠÙ…Ø©
                print(f"File {filename} deleted because it doesn't cover up to yesterday.")
                return None, False  # Ø¥Ø±Ø¬Ø§Ø¹ None Ù…Ø¹ False Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø§Ù‚ØµØ©

            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù…Ù„Ø© Ø­ØªÙ‰ ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ù…Ø³ØŒ Ø­ÙØ¸ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ (close)
            yesterday_close = float(last_line.split(',')[5])  # Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¥ØºÙ„Ø§Ù‚
            return yesterday_close, True  # Ø¥Ø±Ø¬Ø§Ø¹ Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ ÙˆTrue Ù„Ù„Ø¯Ù„Ø§Ù„Ø© Ø¹Ù„Ù‰ Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§Ù…Ù„Ø©

    except Exception as e:
        print(f"An error occurred while checking file {filename}: {e}")
        return None, False  # ÙÙŠ Ø­Ø§Ù„Ø© Ø­Ø¯ÙˆØ« Ø®Ø·Ø£ØŒ Ø¥Ø±Ø¬Ø§Ø¹ None ÙˆFalse

def add_future_dates(filename, symbol):
    today = datetime.now(pytz.utc).replace(hour=0, minute=0, second=0, microsecond=0)
    future_dates = [today + timedelta(days=i) for i in range(3)]  # today and the next two days

    with open(filename, 'a', newline='') as file:
        writer = csv.writer(file)
        for future_date in future_dates:
            formatted_date = future_date.strftime('%Y-%m-%d 00:00:00+00:00')
            writer.writerow([formatted_date, symbol,'1','1','1','1','1'])


data_folder = 'C:/Users/moham/Downloads/crypto/crypto/data'

if not os.path.exists(data_folder):
    os.makedirs(data_folder)

data_filename = os.path.join(data_folder,'data1.csv')

def fetch_and_save_data(symbol, start_date, end_date):
    url = "https://api.binance.com/api/v3/klines"  # Add API URL here
    params = {
        'symbol': symbol,
        'interval': '1d',
        'startTime': int(start_date.timestamp() * 1000),
        'endTime': int(end_date.timestamp() * 1000)
    }
    
    response = requests.get(url, params=params)
    data = response.json()

    if isinstance(data, dict) and 'code' in data and data['code'] == -1121: 
        return False
    
    # Check if data is available
    if not data:
        print(f"No data available for {symbol} from {start_date.date()}")
        return False  # No data for this symbol

    # Write data to the file
    with open(data_filename, 'a', newline='') as file:
        writer = csv.writer(file)
        for entry in data:
            timestamp = datetime.fromtimestamp(entry[0] / 1000, tz=pytz.utc).strftime('%Y-%m-%d 00:00:00+00:00')
            writer.writerow([
                timestamp, symbol, entry[1], entry[2], entry[3], entry[4], entry[5]
            ])
    
    return True  # Data fetchedÂ successfully

def train(cfg: DictConfig):
    start_date = datetime(2020, 1, 1, tzinfo=pytz.utc)
    end_date = datetime.now(pytz.utc) - timedelta(days=1)
    period = timedelta(days=90)

    title = ''
    increase_threshold = 0.03
    saved_percentage=0
  
    for symbol in symbols:
     
        
        # //
        if os.path.exists(data_filename):
            os.remove(data_filename)  # Delete the file if it exists

        # Write headers only when creating the file
        with open(data_filename, 'a', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(['timestamp', 'symbol', 'open', 'high', 'low', 'close', 'volume'])

        current_start = start_date
        data_available = False

        while current_start < end_date:
            current_end = min(current_start + period, end_date)
            print(f"Fetching data for {symbol} from {current_start.date()} to {current_end.date()}")
        
            # Fetch data for each period
            if fetch_and_save_data(symbol, current_start, current_end):
                data_available = True  # Data fetched successfully for at least one period

            current_start = current_end + timedelta(days=1)
    
        if not data_available:
            print(f"Skipping {symbol} due to insufficient data.")
            continue  # Skip symbol if no data is available

   
        yesterday_close, data_complete = check_and_delete_file(data_filename)
        if not data_complete:
            continue
        # Add three future dates at the end of the file with the symbol

        add_future_dates(data_filename, symbol)

        print("Data download complete for all symbols with data from the beginning of 2020.")
        # //
        if cfg.load_path is None and cfg.model is None:
            msg = 'either specify a load_path or config a model.'
            logger.error(msg)
            raise Exception(msg)

        elif cfg.load_path is not None:
            dataset_ = pd.read_csv(cfg.load_path)
            if 'Date' not in dataset_.keys():
                dataset_.rename(columns={'timestamp': 'Date'}, inplace=True)
            if 'High' not in dataset_.keys():
                dataset_.rename(columns={'high': 'High'}, inplace=True)
            if 'Low' not in dataset_.keys():
                dataset_.rename(columns={'low': 'Low'}, inplace=True)

            dataset, profit_calculator = preprocess(dataset_, cfg, logger)

        elif cfg.model is not None:
            dataset, profit_calculator = get_dataset(cfg.dataset_loader.name, cfg.dataset_loader.train_start_date,
                                cfg.dataset_loader.valid_end_date, cfg)

        cfg.save_dir = os.getcwd()
        reporter = Reporter(cfg)
        reporter.setup_saving_dirs(cfg.save_dir)
        model = MODELS[cfg.model.type](cfg.model)

        dataset_for_profit = dataset.copy()
        dataset_for_profit.drop(['prediction'], axis=1, inplace=True)
        dataset.drop(['predicted_high', 'predicted_low'], axis=1, inplace=True)
     
        if cfg.validation_method == 'simple':
            train_dataset = dataset[
                (dataset['Date'] > cfg.dataset_loader.train_start_date) & (
                            dataset['Date'] < cfg.dataset_loader.train_end_date)]
            valid_dataset = dataset[
                (dataset['Date'] > cfg.dataset_loader.valid_start_date) & (
                            dataset['Date'] < cfg.dataset_loader.valid_end_date)]
            Trainer(cfg, train_dataset, None, model).train()
            mean_prediction = Evaluator(cfg, test_dataset=valid_dataset, model=model, reporter=reporter).evaluate()
          
        elif cfg.validation_method == 'cross_validation':
            n_split = 3
            tscv = TimeSeriesSplit(n_splits=n_split)

            for train_index, test_index in tscv.split(dataset):
                train_dataset, valid_dataset = dataset.iloc[train_index], dataset.iloc[test_index]
                Trainer(cfg, train_dataset, None, model).train()
                mean_prediction = Evaluator(cfg, test_dataset=valid_dataset, model=model, reporter=reporter).evaluate()

            reporter.add_average()
        
        x = ProfitCalculator(cfg, dataset_for_profit, profit_calculator, mean_prediction, reporter).profit_calculator()
        predicted_high = x[0]['predicted_high'].iloc[0]
        predicted_low = x[0]['predicted_low'].iloc[0]
        predicted_mean = x[0]['predicted_mean'].iloc[0]
        predicted_high_formated="{:.18f}".format(predicted_high)
        predicted_low_formated="{:.18f}".format(predicted_low)
        predicted_mean_formated="{:.18f}".format(predicted_mean)
        increase = (predicted_mean - yesterday_close) / yesterday_close
        if increase > increase_threshold:
           saved_percentage = increase * 100
        else:
            continue 
        title += f'Ø±Ù…Ø² Ø§Ù„Ø¹Ù…Ù„Ø©: {symbol}\n'
        title += f'Ù†Ø³Ø¨Ø© Ø§Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {round(saved_percentage, 1)}%\n'
        title += f'Ø§Ø¹Ù„Ù‰ Ø³Ø¹Ø± Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„ÙŠÙˆÙ…â¬†ï¸:\n {predicted_high_formated}\n'
        title += f'Ø§Ù‚Ù„ Ø³Ø¹Ø± Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„ÙŠÙˆÙ…â¬‡ï¸:\n {predicted_low_formated}\n'
        title += f'Ø³Ø¹Ø± Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„ÙŠÙˆÙ…:\n {predicted_mean_formated}\n'
        title += '-------------------------------------------------------\n'
        print('..............................d')
        print(yesterday_close)
        reporter.print_pretty_metrics(logger)
        reporter.save_metrics()

    title += 'Ù„Ø§ ØªØ¬Ø¹Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ø­ÙˆØ± ØªØ¯Ø§ÙˆÙ„Ùƒ. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±ØŒ ÙˆØ§Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ù„Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø±Ø§Øª Ù…Ø³ØªÙ†ÙŠØ±Ø©.\n'
    print(title)

    return title  # Return the title or any other relevant data


# @hydra.main(config_path=HYDRA_PATH, config_name="train")
# def main(cfg: DictConfig):
#     result = train(cfg)  # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©
#     print(result)

# ////
from telegram import Update, KeyboardButton, ReplyKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

TOKEN = '7247002552:AAFfzqoRJ95XmwOLDB6Pn2etQTSCU3zT4Pc'

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙ‚Ø¹ (data)
import logging
from telegram import Update, KeyboardButton, ReplyKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import hydra
from omegaconf import DictConfig
from functools import partial

# Ø¥Ø¹Ø¯Ø§Ø¯ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
logging.basicConfig(level=logging.INFO)

TOKEN = "7247002552:AAFfzqoRJ95XmwOLDB6Pn2etQTSCU3zT4Pc"  # Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ù€ ØªÙˆÙƒÙ† Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ
# HYDRA_PATH = "C:\Users\moham\Downloads\crypto\crypto\configs"  # Ù…Ø³Ø§Ø± Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†

async def data(update: Update, context: ContextTypes.DEFAULT_TYPE, cfg: DictConfig) -> None:
    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ "ØªÙˆÙ‚Ø¹"
    result = train(cfg)  # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© train Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… cfg
    await update.message.reply_text(result)

# Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªÙ‚ÙˆÙ… Ø¨Ø¹Ø±Ø¶ Ø²Ø± "ØªÙˆÙ‚Ø¹" ÙƒØ²Ø± Ø¯Ø§Ø¦Ù…
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø²Ø± "ØªÙˆÙ‚Ø¹" ÙƒØ²Ø± Ø¯Ø§Ø¦Ù…
    keyboard = [[KeyboardButton("ØªÙˆÙ‚Ø¹")]]
    reply_markup = ReplyKeyboardMarkup(keyboard, resize_keyboard=True, one_time_keyboard=False)
    
    await update.message.reply_text('Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø± Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©.', reply_markup=reply_markup)

# Ø¯Ø§Ù„Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ "ØªÙˆÙ‚Ø¹"
async def handle_prediction(update: Update, context: ContextTypes.DEFAULT_TYPE, cfg: DictConfig) -> None:
    if update.message.text == "ØªÙˆÙ‚Ø¹":
        await data(update, context, cfg)  # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙˆÙ‚Ø¹ Ù…Ø¹ ØªÙ…Ø±ÙŠØ± cfg

@hydra.main(config_path=HYDRA_PATH, config_name="train")
def main(cfg: DictConfig) -> None:
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    application = Application.builder().token(TOKEN).build()
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£ÙˆØ§Ù…Ø± ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
    application.add_handler(CommandHandler('start', start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, partial(handle_prediction, cfg=cfg)))  # ØªÙ…Ø±ÙŠØ± cfg Ù‡Ù†Ø§
    
    # Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª
    application.run_polling()

if __name__ == '__main__':
    main()


